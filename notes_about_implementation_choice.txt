- tide embeddings or not ?

Regularizations
----------------
- weight decay : no overfitting by minimising the norm of the parameters.
    > So no need to minimize the regularization modules parameters (LayerNorm) et and the biaises
    since those latter don't contribute to the separability of the training data.
    > weight decay only on slopes of Linear modules

Stability
---------
- scaling some parameters
    > no w_decay on embeddings but perhaps some scaling in order to deal with frequent words ?
- initialization