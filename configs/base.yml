add_positions: true
epochs: 1
batch_size: 4
dev: ../fula.txt
dropout: 0.1
embedding_dims: 768
ff_size: 3072
heads: 12
layers: 12
lr: 2.5e-4
max_length: 200
pad_idx: 3
tokenizer: data/tokenizers/fula.model
train: ../fula.txt
gradients_accumulation: 256
vocab_size: 24000
valid_every_n_steps: 17000
T_max: 2000
checkpoint: "checkpoints/fula.pt"