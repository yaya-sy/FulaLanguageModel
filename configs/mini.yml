add_positions: true
epochs: 10
batch_size: 16
dev: data/raw_text/fula.txt
dropout: 0.07
embedding_dims: 128
ff_size: 256
heads: 4
layers: 2-4
lr: 0.000013
max_length: 200
pad_idx: 3
tokenizer: data/tokenizer_models/fula.model
train: data/raw_text/fula.txt
vocab_size: 12000
print_generation_steps: 5000
checkpoint: "checkpoints/fula.pt"