add_positions: true
epochs: 50
batch_size: 16
dev: data/raw_text/fula.txt
dropout: 0.07
embedding_dims: 128
ff_size: 512
heads: 4
layers: 4
lr: 0.00013
max_length: 200
pad_idx: 3
tokenizer: data/tokenizer_models/fula.model
train: data/raw_text/fula.txt
vocab_size: 12000
device: cpu
