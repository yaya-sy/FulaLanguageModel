add_positions: true
epochs: 10
batch_size: 16
dev: data/raw_text/fula.txt
dropout: 0.1
embedding_dims: 128
ff_size: 256
heads: 2
layers: 2
lr: 0.0003
max_length: 200
pad_idx: 3
tokenizer: data/tokenizer_models/fula.model
train: data/raw_text/fula.txt
vocab_size: 12000
print_generation_steps: 5000